{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c382d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 18:50:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/hadoop/spark\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName = \"tomato\")\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab59af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import os\n",
    "\n",
    "# 새로운 SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Tomato Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 경로 설정\n",
    "data_path=\"hdfs://master01:9000/user/hadoop/strawberry/\"\n",
    "# CSV 파일 리스트\n",
    "csv_files = [\n",
    "    \"STRAWBERRY_ACIDITY_ENV_20221212.csv\",\n",
    "    \"STRAWBERRY_FLOWER_NUM_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_FLOWERLESS_NUM_ENV_20221210.csv\",\n",
    "    \"STRAWBERRY_FLOWER_NUM_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_FRUIT_LEN_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_FRUIT_QUALITY_INFO_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_FRUIT_QUANTITY_INFO_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_FRUIT_SETTING_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_FRUIT_WEIGHT_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_FRUIT_WIDTH_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_GROWTH_LENGTH_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_LEAF_INFO_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_LEAF_LEN_ENV_20221203.csv\",\n",
    "    \"STRAWBERRY_LEAF_NUM_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_LEAF_WIDTH_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_PETIOLE_LEN_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_PRODUCTION_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_ROOTING_SEASON_ENV_20221203.csv\",\n",
    "    \"STRAWBERRY_ROOT_NUM_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_SOIL_SURFACE_LEN_ENV_20221202.csv\",\n",
    "    \"STRAWBERRY_STEM_INFO_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_SUGAR_CONTENT_ENV_20221209.csv\",\n",
    "    \"STRAWBERRY_THECA_DIAMETER_ENV_20221209.csv\"\n",
    "]\n",
    "\n",
    "# 모든 파일에서 열 이름 수집\n",
    "all_columns = set()\n",
    "for file in csv_files:\n",
    "    df = spark.read.csv(data_path+file, header=True, inferSchema=True)\n",
    "    all_columns.update(df.columns)\n",
    "\n",
    "all_columns = list(all_columns)\n",
    "all_columns.remove(\"STRG_DT\")  # MSRM_DT는 중복으로 처리하지 않음\n",
    "all_columns = [\"STRG_DT\"] + all_columns\n",
    "\n",
    "# 각 파일을 읽어와서 데이터 채우기\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    df = spark.read.csv(os.path.join(data_path, file), header=True, inferSchema=True)\n",
    "    df = df.drop(\"ZONE_NM\")  # ZONE_NM 열 제거\n",
    "    for col in all_columns:\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, F.lit(None).cast(T.StringType()))\n",
    "    dataframes.append(df.select(all_columns))\n",
    "\n",
    "# 모든 데이터프레임 병합\n",
    "merged_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    merged_df = merged_df.union(df)\n",
    "\n",
    "# 중복된 열에 대한 평균 계산\n",
    "agg_exprs = []\n",
    "for col in merged_df.columns:\n",
    "    if col != \"STRG_DT\":\n",
    "        agg_exprs.append(F.mean(col).alias(col))\n",
    "\n",
    "final_df = merged_df.groupBy(\"STRG_DT\").agg(*agg_exprs)\n",
    "final_df_sorted = final_df.orderBy(\"STRG_DT\")\n",
    "\n",
    "# 통합된 데이터 저장\n",
    "output_path = \"hdfs://master01:9000/user/hadoop/test_merge_strawberry_data.csv\"\n",
    "final_df_sorted.coalesce(1).write.csv(output_path, header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f014a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
